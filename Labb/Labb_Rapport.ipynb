{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b9a330",
   "metadata": {},
   "source": [
    "<h1>Linjär reggression</h1>\n",
    "<h2>Analys av california housing dataset</h2>  \n",
    "<em>Adrian Söderberg Skog</em><br>\n",
    "<strong>AIMG25</strong>\n",
    "<h3>Importer</h3>\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "317d9866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'LinearRegression' from 'c:\\\\Users\\\\adria\\\\Desktop\\\\Python\\\\Statistiska_Metoder\\\\Labb\\\\LinearRegression.py'>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import LinearRegression as lreg\n",
    "import importlib\n",
    "importlib.reload(lreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f1da9",
   "metadata": {},
   "source": [
    "<h3>Ladda in och inspektera dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9b7d9931",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'housing.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[175]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhousing.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m df.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\Desktop\\Python\\Statistiska_Metoder\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\Desktop\\Python\\Statistiska_Metoder\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\Desktop\\Python\\Statistiska_Metoder\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\Desktop\\Python\\Statistiska_Metoder\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adria\\Desktop\\Python\\Statistiska_Metoder\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'housing.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"housing.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb543fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           20640 non-null  float64\n",
      " 1   latitude            20640 non-null  float64\n",
      " 2   housing_median_age  20640 non-null  float64\n",
      " 3   total_rooms         20640 non-null  float64\n",
      " 4   total_bedrooms      20433 non-null  float64\n",
      " 5   population          20640 non-null  float64\n",
      " 6   households          20640 non-null  float64\n",
      " 7   median_income       20640 non-null  float64\n",
      " 8   median_house_value  20640 non-null  float64\n",
      " 9   ocean_proximity     20640 non-null  object \n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc4d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude               0\n",
       "latitude                0\n",
       "housing_median_age      0\n",
       "total_rooms             0\n",
       "total_bedrooms        207\n",
       "population              0\n",
       "households              0\n",
       "median_income           0\n",
       "median_house_value      0\n",
       "ocean_proximity         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce603ebc",
   "metadata": {},
   "source": [
    "<h3>Bearbetning av data</h3>\n",
    "<p>Datasetet innehåller 207 NaN värden i kolumnen \"total bedrooms\". Eftersom det är en liten del av de totala antalet rader på 20640, gör jag bedömningen att det är relativt säkert att ta bort dessa. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594423b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c20c6e9",
   "metadata": {},
   "source": [
    "\n",
    "<p>Vi kan också se att det finns en kolumn med kategoriska värden. För att kunna inkludera dessa i reggressionen behöver de kodas om till numeriska värden. Eftersom de inte har någon tydlig hirarkisk struktur gör jag bedömningen att det lämpligaste är att använda one hot encoding. I min kod fil har jag därför också gjort en klass oneHotEncoding som kan omvandla den kategoriska kolumnen till nya kolumner med binära värden. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3084b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = lreg.OneHotEncoder()\n",
    "\n",
    "y = df_clean.iloc[:,8].to_numpy()\n",
    "X = pd.concat([df_clean.iloc[:,:8], df_clean.iloc[:,-1]], axis=1) \n",
    "\n",
    "X_enc = encoder.fit_transform(X, column_names=df_clean.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a061485",
   "metadata": {},
   "source": [
    "<p>Sista steget innan vi kan göra en linjär regressionsanalys på datasetet är att det behöver delas upp i träningsdata och testdata samt att målvariabeln behöver separeras från de övriga parametrarna. I kodfilen har jag gort en funktion som slumpmässigt delar upp punkterna. Defaultuppdelningen är 80% i träningsdata och 20% i testdata. </p>\n",
    "<p>Målvariabeln kommer att vara median house value, eftersom det är huspriser vi vill kunna förutsäga med hjälp av de andra parametrarna. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.column_stack((X_enc, y))\n",
    "\n",
    "train_Data, test_Data = lreg.train_test_split(data)\n",
    "\n",
    "train_X, train_y = train_Data[:,:-1], train_Data[:,-1]\n",
    "test_X, test_y = test_Data[:,:-1], test_Data[:,-1]\n",
    "\n",
    "feature_names = encoder.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2f667",
   "metadata": {},
   "source": [
    "<h3>Träning av modellen</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8331e1d",
   "metadata": {},
   "source": [
    "<p>Fit metoden i LinearRegression klassen upskattar koefficienterna med hjälp av minsta kvadratmetoden. Istället för den vanliga formeln <br> &beta; = (X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>Y <br> valde jag att använda Moore Penrose pseudoinvers som kan skrivas som X<sup>+</sup> = (X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup> vilket ger att &beta; = X<sup>+</sup>Y <br>\n",
    "Fördelen med den är att den fungerar även i situationer där inversen av X<sup>T</sup>X inte är deffinerad, vilket kan uppstå till exempel om det finns multikoliniäritet mellan parametrar eller om X har fler parametrar än rader. <br>\n",
    "Fit metoden returnerar sedan koefficientmatrisen och interceptet. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc92e50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.60234928e+04, -2.48095574e+04,  1.07863193e+03, -6.00828177e+00,\n",
       "         1.00932590e+02, -3.88607446e+01,  5.19530484e+01,  3.92730524e+04,\n",
       "        -3.95229821e+04,  2.36847146e+05, -5.14890598e+03,  3.70319487e+03]),\n",
       " np.float64(-2200643.7424594937))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lreg.LinearRegression()\n",
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b09dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression summary\n",
      "==========================\n",
      "Target variable:            median house value\n",
      "Sample size:                16346\n",
      "Number of features:         12\n",
      "\n",
      "variance:                   4705120588.7065\n",
      "R square:                   0.6457\n",
      "Adjusted R square:          0.6455\n",
      "train RMSE:                 68593.8816\n",
      "test RMSE:                  69018.2787\n",
      "F-statistic:                2480.6957\n",
      "p value (F-statistic):      0.0000\n",
      "\n",
      "Confidence level:   95.0%\n",
      "Feature                 CI lower   coefficient   CI upper  t-value   p-value   significant\n",
      "==========================================================================================\n",
      "longitude                  -28254.50 -26023.49 -23792.49    -22.86      0.00       Yes\n",
      "latitude                   -27010.84 -24809.56 -22608.28    -22.09      0.00       Yes\n",
      "housing_median_age            982.43   1078.63   1174.83     21.98      0.00       Yes\n",
      "total_rooms                    -7.74     -6.01     -4.27     -6.79      0.00       Yes\n",
      "total_bedrooms                 85.44    100.93    116.42     12.77      0.00       Yes\n",
      "population                    -41.23    -38.86    -36.49    -32.14      0.00       Yes\n",
      "households                     35.21     51.95     68.70      6.08      0.00       Yes\n",
      "median_income               38534.04  39273.05  40012.07    104.17      0.00       Yes\n",
      "median_house_value_INLAND  -43345.26 -39522.98 -35700.71    -20.27      0.00       Yes\n",
      "median_house_value_ISLAND  141715.79 236847.15 331978.50      4.88      0.00       Yes\n",
      "median_house_value_NEAR BAY  -9339.44  -5148.91   -958.37     -2.41      0.02       Yes\n",
      "median_house_value_NEAR OCEAN    257.72   3703.19   7148.67      2.11      0.04       Yes\n"
     ]
    }
   ],
   "source": [
    "model.summary(feature_names=feature_names, \n",
    "              target_name=\"median house value\", \n",
    "              testX= test_X, \n",
    "              testy= test_y, \n",
    "              CL=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871a573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation:\n",
      "\n",
      "longitude                  [' 1.00', '-0.92', '-0.11', ' 0.05', ' 0.07', ' 0.10', ' 0.06', '-0.02', '-0.06', ' 0.01', '-0.47', ' 0.05', '-0.05']\n",
      "latitude                   ['-0.92', ' 1.00', ' 0.01', '-0.04', '-0.07', '-0.11', '-0.07', '-0.08', ' 0.35', '-0.02', ' 0.36', '-0.16', '-0.14']\n",
      "housing_median_age         ['-0.11', ' 0.01', ' 1.00', '-0.36', '-0.32', '-0.30', '-0.30', '-0.12', '-0.24', ' 0.02', ' 0.26', ' 0.02', ' 0.11']\n",
      "total_rooms                [' 0.05', '-0.04', '-0.36', ' 1.00', ' 0.93', ' 0.86', ' 0.92', ' 0.20', ' 0.03', '-0.01', '-0.02', '-0.01', ' 0.13']\n",
      "total_bedrooms             [' 0.07', '-0.07', '-0.32', ' 0.93', ' 1.00', ' 0.88', ' 0.98', '-0.01', '-0.01', '-0.00', '-0.02', ' 0.00', ' 0.05']\n",
      "population                 [' 0.10', '-0.11', '-0.30', ' 0.86', ' 0.88', ' 1.00', ' 0.91', ' 0.01', '-0.02', '-0.01', '-0.06', '-0.02', '-0.03']\n",
      "households                 [' 0.06', '-0.07', '-0.30', ' 0.92', ' 0.98', ' 0.91', ' 1.00', ' 0.01', '-0.04', '-0.01', '-0.01', ' 0.00', ' 0.06']\n",
      "median_income              ['-0.02', '-0.08', '-0.12', ' 0.20', '-0.01', ' 0.01', ' 0.01', ' 1.00', '-0.24', '-0.01', ' 0.06', ' 0.03', ' 0.69']\n",
      "median_house_value_INLAND  ['-0.06', ' 0.35', '-0.24', ' 0.03', '-0.01', '-0.02', '-0.04', '-0.24', ' 1.00', '-0.01', '-0.24', '-0.26', '-0.48']\n",
      "median_house_value_ISLAND  [' 0.01', '-0.02', ' 0.02', '-0.01', '-0.00', '-0.01', '-0.01', '-0.01', '-0.01', ' 1.00', '-0.01', '-0.01', ' 0.02']\n",
      "median_house_value_NEAR BAY ['-0.47', ' 0.36', ' 0.26', '-0.02', '-0.02', '-0.06', '-0.01', ' 0.06', '-0.24', '-0.01', ' 1.00', '-0.14', ' 0.16']\n",
      "median_house_value_NEAR OCEAN [' 0.05', '-0.16', ' 0.02', '-0.01', ' 0.00', '-0.02', ' 0.00', ' 0.03', '-0.26', '-0.01', '-0.14', ' 1.00', ' 0.14']\n",
      "median house value         ['-0.05', '-0.14', ' 0.11', ' 0.13', ' 0.05', '-0.03', ' 0.06', ' 0.69', '-0.48', ' 0.02', ' 0.16', ' 0.14', ' 1.00']\n"
     ]
    }
   ],
   "source": [
    "names = feature_names + [\"median house value\"]\n",
    "model.corr_matrix(data, column_names=names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836b2233",
   "metadata": {},
   "source": [
    "<h3>Resultat och disskusion</h3>\n",
    "<p>\n",
    "Överlag kan man konstatera att reggressionen är signifikant. Vi har en förklaringsgrad på nära 65% dvs. modellen kan förklara 65% av variationen i huspriserna, vilket är ganska okej med tanke på att det finns många fler faktorer som kan påverka huspriser. R<sup>2</sup> och adjusted R<sup>2</sup> är i princip identistka vilket tyder på att vi inte har övertränat med onödiga parametrar. Tränings RMSE och test RMSE är också relativt lika vilket också indikerar att modellen generaliserar bra. Modellen missar i snitt med 68000$. <br>\n",
    "Jag har valt en konfidensnivå på 95%. Det finns ingen ingen anledning att vara striktare än så vid estimering av fastighetspriser. Detta innebär att vi har en signifikansnivå på 5% \n",
    "F testet gav ett p värde som är väldigt nära 0, långt under 0,05 vilket gör att vi med säkerhet kan avfärda nollhypetesen. Med andra ord så vet vi att minst en av parametrarna har en signifikant effekt. Kollar vi på de individuella parametrarna så ser vi också att det bara är NEAR BAY och NEAR OCEAN som är i riskzoonen för att inte vara signiffikanta där NEAR OCEAN hamnade precis under 0,05.<br> Vår bästa predikator är median income som har högst signiffikans. Vi kan också se i korrelationsmatrisen att den har en korrelation med median house value på 0.69. Det är inte helt oväntat eftersom folk med höre inkomst i större utsträckning kan köpa dyrare hus. Det är också intressant att antalet totala rum har en negativ effekt på priset men totala antalet sovrum har en positiv effekt. Men eftersom det är totala antalet rum i ett område så kan man tänka sig att ett stort antal rum också kan innebära många små hus som tenderar att vara billigare. <br>\n",
    "</p>\n",
    "<p>\n",
    "I korrelationsmatrisen kan vi se tendenser till multikoliniäritet mellan parametrarna total rooms, total bedrooms, population och households. I detta läge kanske man skulle valt ut en av dessa som är mest relevant, men användandet av Moore Penrose pseudoinvers skall hjälpa till att få stabilare uträkning av koefficienter i dessa lägen. Hade vi använt den vanliga formeln så hade vi förmodligen sett större varians i kofficienterna. \n",
    "</p>\n",
    "<p>\n",
    "Sammanfattningsvis kan vi konstatera att modellen kan ge relevanta prediktioner. Träningsdatan består av 16346 rader, vilket är tillräkligt för att kunna göra en bra generalisering, och det såg vi också i resultatet. För att lättare kunna gämföra parametrarnas signifikans hade det varit födelaktigt att normalisera datan innan. Det tar jag med mig till nästa gång.    \n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
